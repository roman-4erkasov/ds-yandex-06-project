{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ayx-MU9QwENi",
    "outputId": "74a0cef1-7a89-4e74-8285-3a3de99cfc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# примонтировать данные с гугл-диска\n",
    "# пи запуске с локальной машины это не нужно\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e_Xpf6olxGiD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/roman/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.util import skipgrams\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import BernoulliNB, BaseNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import nltk.data\n",
    "nltk.download('popular')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5g6Fv7ASxGiJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dpred= pd.read_csv(\"/content/gdrive/My Drive/NLP/works/wrk01_sentiment/product-reviews-sentiment-analysis-light/products_sentiment_test.tsv\",sep=\"\\t\")\n",
    "\n",
    "# dtrain = pd.read_csv(\n",
    "#     \"/content/gdrive/My Drive/NLP/works/wrk01_sentiment/product-reviews-sentiment-analysis-light/products_sentiment_train.tsv\", \n",
    "#     sep=\"\\t\",\n",
    "#     header=None,\n",
    "#     names=[\"text\", \"response\"]\n",
    "# )\n",
    "\n",
    "dpred= pd.read_csv(\"../product-reviews-sentiment-analysis-light/products_sentiment_test.tsv\",sep=\"\\t\")\n",
    "\n",
    "dtrain = pd.read_csv(\n",
    "    \"../product-reviews-sentiment-analysis-light/products_sentiment_train.tsv\", \n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"text\", \"response\"]\n",
    ")\n",
    "\n",
    "cv = RepeatedKFold(n_repeats=4, n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ua8TAbXZxGiQ"
   },
   "source": [
    "# Simple Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1j17fhiyxGiQ",
    "outputId": "e20474c9-a2df-425e-ace4-2dc1e1c5138f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg=0.7541249999999999 std=0.015717724867168267\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('vec', CountVectorizer(stop_words=\"english\")),\n",
    "        ('clf', LogisticRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores = cross_val_score(pipe, dtrain.text, dtrain.response, cv=cv)\n",
    "print(f\"avg={scores.mean()} std={scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ccw_SwkxxGiU",
    "outputId": "c8015785-449e-48c2-a291-47b57124e9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg=0.76485 std=0.012527070687115975\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('vec', TfidfVectorizer(ngram_range=(1, 2))), #avg=0.7444 std=0.012\n",
    "#         ('vec', TfidfVectorizer(stop_words=\"english\")), #avg=0.7179 std=0.018\n",
    "        ('clf', LogisticRegressionCV(max_iter=1000, cv=RepeatedKFold(n_repeats=2, n_splits=10)))\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores = cross_val_score(pipe, dtrain.text, dtrain.response, cv=RepeatedKFold(n_repeats=10, n_splits=2))\n",
    "print(f\"avg={scores.mean()} std={scores.std()}\")\n",
    "# avg=0.7646000000000001 std=0.010016985574512931\n",
    "# avg=0.7659 std=0.013018832512940639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7lz05D-xGio",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### for feature analysis ######\n",
    "\n",
    "# dtrain['pred'] = pred[:, 1]\n",
    "# dtrain['aerr'] = abs(dtrain.pred - dtrain.response)\n",
    "\n",
    "# dtrain.sort_values(by='aerr', ascending=False)\n",
    "\n",
    "# pipe.fit(dtrain.text, dtrain.response)\n",
    "\n",
    "# vec = pipe.named_steps['vec']\n",
    "# res = vec.transform([dtrain.text[15]]).todense()\n",
    "# list(filter(lambda x: x[1]!=0, zip(vec.get_feature_names(),res.tolist()[0])))\n",
    "\n",
    "# def get_wei(word):\n",
    "#     word_idx = pipe.named_steps['vec'].vocabulary_.get(word, None)\n",
    "#     return pipe.named_steps['clf'].coef_[0][word_idx] if word_idx is not None else None\n",
    "\n",
    "# txt = dtrain.text[488].split()\n",
    "# vec = pipe.named_steps['vec'].transform([dtrain.text[15]]).todense()\n",
    "# dict(zip(txt,list(map(get_wei, txt))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d0vBbAM4z-1A",
    "outputId": "becf4a46-e4fc-46ec-eeb8-d58ef80b8160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg=0.75 std=0.030145480589965735\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('vec', TfidfVectorizer()),#stop_words=\"english\")),\n",
    "        ('clf', BernoulliNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores = cross_val_score(pipe, dtrain.text, dtrain.response, cv=RepeatedKFold(n_repeats=4, n_splits=10))\n",
    "print(f\"avg={scores.mean()} std={scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tqykwM2SsoY"
   },
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qx7sQ3WJbPVR"
   },
   "source": [
    "## vesrion 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCZNhDJFSwAh"
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         ('vec', TfidfVectorizer(stop_words=\"english\")),\n",
    "#         ('clf', LogisticRegression())\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# pipe.fit(dtrain.text, dtrain.response)\n",
    "# dresult = pd.DataFrame({\n",
    "#     \"Id\": dpred.Id,\n",
    "#     \"y\": pipe.predict(dpred.text)\n",
    "# })\n",
    "# dresult.to_csv(\"tfidf_lr.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zRmfBv0bTRY"
   },
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLinMii4blWO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...    random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('vec', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "        ('clf', LogisticRegressionCV(max_iter=500))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(dtrain.text, dtrain.response)\n",
    "# dresult = pd.DataFrame({\n",
    "#     \"Id\": dpred.Id,\n",
    "#     \"y\": pipe.predict(dpred.text)\n",
    "# })\n",
    "# dresult.to_csv(\"tfidf_lr_2.csv\", index=None)\n",
    "\n",
    "# 0.825003 on LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vec.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe.named_steps['vec'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clf.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe.named_steps['clf'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vec.pkl', 'rb') as f:\n",
    "    data_new = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x22786 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.transform([\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6cbd2Lridf1"
   },
   "source": [
    "## version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dN76NEzhcYxw"
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         ('vec', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "#         ('clf', LogisticRegressionCV(max_iter=500, cv=RepeatedKFold(n_repeats=2, n_splits=10)))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# pipe.fit(dtrain.text, dtrain.response)\n",
    "# dresult = pd.DataFrame({\n",
    "#     \"Id\": dpred.Id,\n",
    "#     \"y\": pipe.predict(dpred.text)\n",
    "# })\n",
    "# dresult.to_csv(\"tfidf_lr_3.csv\", index=None)\n",
    "\n",
    "# # 0.81750 on LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-i_j7Pai6zY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "w02_sa.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
