{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import bs4\n",
    "import re\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pymystem3\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словари\n",
    "negation = {\"не\"}\n",
    "stopwords = {'c',  'а',  'але',  'без',  'белый',  'близко',  'более',  'больше',  'большой',  'будто',  'бы',  'бывать',  'бывь',  'быть',  'в',  'важный',  'ваш',  'вверх',  'вдали',  'вдруг',  'ведь',  'везде',  'вернуться',  'весь',  'вечер',  'взгляд',  'взять',  'вид',  'видеть',  'вместе',  'вне',  'вниз',  'внизу',  'во',  'вода',  'война',  'вокруг',  'вон',  'вообще',  'вопрос',  'восемнадцатый',  'восемнадцать',  'восемь',  'восьмой',  'вот',  'впрочем',  'время',  'все',  'всегда',  'всего',  'всюду',  'второй',  'вы',  'выходить',  'г',  'где',  'главный',  'глаз',  'говорить',  'год',  'голова',  'голос',  'город',  'да',  'давать',  'давно',  'даже',  'далекий',  'далеко',  'даром',  'два',  'двадцатый',  'двадцать',  'двенадцатый',  'двенадцать',  'дверь',  'девятнадцатый',  'девятнадцать',  'девятый',  'девять',  'действительно',  'делать',  'дело',  'день',  'деньги',  'десятый',  'десять',  'для',  'до',  'довольно',  'долго',  'должно',  'должный',  'дом',  'дорога',  'друг',  'друго',  'другой',  'думать',  'душа',  'е',  'если',  'еще',  'ж',  'ждать',  'же',  'жена',  'женщина',  'жизнь',  'жить',  'за',  'занимать',  'занятый',  'затем',  'зато',  'зачем',  'здесь',  'земля',  'знать',  'значит',  'значить',  'и',  'идти',  'из',  'или',  'именно',  'иметь',  'имя',  'иногда',  'к',  'каждый',  'кажется',  'казаться',  'как',  'какой',  'книга',  'когда',  'ком',  'комната',  'конец',  'конечно',  'который',  'кроме',  'кругом',  'кто',  'куда',  'лежать',  'ли',  'лицо',  'лишь',  'любить',  'м',  'маленький',  'мало',  'мать',  'машина',  'между',  'менее',  'место',  'миллион',  'мимо',  'минута',  'мир',  'много',  'многочисленный',  'мож',  'может',  'можно',  'можхо',  'мой',  'молоть',  'мор',  'москва',  'мочь',  'мы',  'на',  'наверху',  'над',  'надо',  'назад',  'наиболее',  'наконец',  'народ',  'находить',  'начинать',  'наш',  'недавно',  'недалеко',  'некоторый',  'нельзя',  'немного',  'немой',  'непрерывный',  'нередко',  'несколько',  'нет',  'ни',  'нибудь',  'ниже',  'низко',  'никакой',  'никогда',  'никто',  'никуда',  'ничего',  'ничто',  'но',  'новый',  'нога',  'ночь',  'ну',  'нужно',  'нужный',  'нх',  'о',  'об',  'оба',  'обычно',  'один',  'одиннадцатый',  'одиннадцать',  'однажды',  'однако',  'оказываться',  'окно',  'около',  'он',  'она',  'они',  'оно',  'опять',  'особенно',  'оставаться',  'от',  'отвечать',  'отец',  'откуда',  'отовсюду',  'отсюда',  'очень',  'первый',  'перед',  'писать',  'плечо',  'по',  'под',  'подумать',  'подходить',  'пожалуйста',  'поздно',  'пойти',  'пока',  'пол',  'получать',  'помнить',  'понимать',  'пора',  'после',  'последний',  'посмотреть',  'посреди',  'потом',  'потому',  'почему',  'почти',  'правда',  'прекрасно',  'при',  'про',  'просто',  'против',  'процент',  'путь',  'пятнадцатый',  'пятнадцать',  'пятый',  'пять',  'работа',  'работать',  'раз',  'разве',  'рано',  'ребенок',  'решать',  'россия',  'рука',  'русский',  'ряд',  'рядом',  'с',  'сам',  'самый',  'свет',  'свое',  'свой',  'сделать',  'сеаой',  'себя',  'сегодня',  'седьмой',  'сей',  'сейчас',  'семнадцатый',  'семнадцать',  'семь',  'сидеть',  'сила',  'сказать',  'сколько',  'слишком',  'слово',  'случай',  'смотреть',  'сначала',  'снова',  'со',  'советский',  'совсем',  'спасибо',  'спрашивать',  'сразу',  'становиться',  'старый',  'стол',  'сторона',  'стоять',  'страна',  'суть',  'считать',  'т',  'так',  'также',  'таки',  'такой',  'там',  'твой',  'теперь',  'то',  'товарищ',  'тогда',  'тоже',  'только',  'том',  'тот',  'третий',  'три',  'тринадцатый',  'тринадцать',  'туда',  'тут',  'ты',  'тысяча',  'у',  'увидеть',  'уж',  'уже',  'улица',  'уметь',  'утро',  'хороший',  'хорошо',  'хотеть',  'хоть',  'хотя',  'час',  'часто',  'часть',  'человек',  'через',  'четвертый',  'четыре',  'четырнадцатый',  'четырнадцать',  'что',  'чтоб',  'чтобы',  'чуть',  'шестнадцатый',  'шестнадцать',  'шестой',  'шесть',  'это',  'этот',  'я',  'являться'}\n",
    "\n",
    "token_list = []\n",
    "rgx_tokens = re.compile(pattern=\"\\w+\")\n",
    "rgx_number = re.compile(\"[\\d\\W\\-]+\")\n",
    "\n",
    "def preprocesing(text):\n",
    "    tokens = rgx_tokens.findall(text)\n",
    "    is_negation = False\n",
    "    result = []\n",
    "    for token in tokens: \n",
    "        morph_info = morph.analyze(token)\n",
    "        if morph_info and morph_info[0].get(\"analysis\", False)\\\n",
    "            and morph_info[0][\"analysis\"] \\\n",
    "            and morph_info[0][\"analysis\"][0].get(\"lex\", False):\n",
    "            lemma = morph_info[0][\"analysis\"][0][\"lex\"]\n",
    "        else:\n",
    "            continue\n",
    "        if lemma in stopwords:\n",
    "            is_negation = False\n",
    "            continue\n",
    "        if token.lower() in negation:\n",
    "            is_negation = True\n",
    "            continue\n",
    "        elif is_negation:\n",
    "            is_negation = False\n",
    "            lemma = \"NEG_\"+lemma\n",
    "        \n",
    "        result.append(lemma)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тестовых данных\n",
    "with open('test.csv') as f:\n",
    "    text = f.read()\n",
    "    parser = bs4.BeautifulSoup(text,'lxml')\n",
    "    txt_test = list(x.get_text() for x in  parser.find_all('review'))\n",
    "    del text\n",
    "    del parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка тренировочных данных\n",
    "\n",
    "with open(\"./reviews01.json\") as f:\n",
    "    txt_train1 = json.load(f)\n",
    "\n",
    "with open(\"./citilink_reviews.json\") as f:\n",
    "    txt_train2 = json.load(f)\n",
    "\n",
    "#разбивка на положительный и отрицательный классы\n",
    "pos_docs = []\n",
    "neg_docs = []\n",
    "for txt_train in [txt_train1, txt_train2]:\n",
    "    for rec in txt_train:\n",
    "        pos_docs.append(rec[\"advantages\"])\n",
    "        neg_docs.append(rec[\"disadvantages\"])\n",
    "        if int(rec[\"rating\"])>3:\n",
    "            pos_docs.append(rec[\"comment\"])\n",
    "        elif int(rec[\"rating\"])<3:\n",
    "            neg_docs.append(rec[\"comment\"])\n",
    "\n",
    "dtrain = pd.concat(\n",
    "    objs=[\n",
    "        pd.DataFrame({\n",
    "            \"text\":pos_docs\n",
    "        }).assign(response=1),\n",
    "        pd.DataFrame({\n",
    "            \"text\":neg_docs\n",
    "        }).assign(response=0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dtest = pd.DataFrame({\n",
    "    \"Id\": list(range(len(txt_test))),\n",
    "    \"text\": txt_test\n",
    "})\n",
    "\n",
    "morph = pymystem3.Mystem()\n",
    "# dtrain[\"text\"] = dtrain.text.apply(preprocesing)\n",
    "# dtest[\"text\"] = dtest.text.apply(preprocesing)\n",
    "\n",
    "dtrain = dtrain[dtrain.text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count,LogisticRegression,uni)=0.8551246713212597\n",
      "(tfidf,LogisticRegression,uni)=0.8502455582614669\n",
      "(count,LogisticRegression,bi) =0.8596577828052405\n",
      "(tfidf,LogisticRegression,bi) =0.8092763769114171\n",
      "\n",
      "(count,SGDClassifier,uni)=0.8345509208131293\n",
      "(tfidf,SGDClassifier,uni)=0.8490243776793098\n",
      "(count,SGDClassifier,bi) =0.8469311979400089\n",
      "(tfidf,SGDClassifier,bi) =0.8697694535638029\n",
      "\n",
      "(count,LinearSVC,uni)=0.8340326820002304\n",
      "(tfidf,LinearSVC,uni)=0.8539048569397348\n",
      "(count,LinearSVC,bi) =0.8427513831827127\n",
      "(tfidf,LinearSVC,bi) =0.8688983728942784\n",
      "\n",
      "(count,BernoulliNB,uni)=0.8397802843486046\n",
      "(tfidf,BernoulliNB,uni)=0.8397802843486046\n",
      "(count,BernoulliNB,bi) =0.679037923794495\n",
      "(tfidf,BernoulliNB,bi) =0.679037923794495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(action='ignore', category=warnings.ConvergenceWarning)\n",
    "token_pattern = r'(?u)\\b\\w\\w+\\b|!|\\?|\\)|\\%'\n",
    "models = [\n",
    "    ('LogisticRegression', LogisticRegression()),\n",
    "#     ('LogisticRegressionCV', LogisticRegressionCV()),\n",
    "    ('SGDClassifier', SGDClassifier()),\n",
    "    ('LinearSVC', LinearSVC()),\n",
    "    (\"BernoulliNB\", BernoulliNB()),\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        count_vec_uni = CountVectorizer(token_pattern=token_pattern) \n",
    "        tfidf_vec_uni = TfidfVectorizer(token_pattern=token_pattern)\n",
    "        count_vec_bi = CountVectorizer(ngram_range=(1, 2)) \n",
    "        tfidf_vec_bi = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "        x_count_uni = count_vec_uni.fit_transform(dtrain['text'])\n",
    "        x_tfidf_uni = tfidf_vec_uni.fit_transform(dtrain['text'])\n",
    "        x_count_bi = count_vec_bi.fit_transform(dtrain['text'])\n",
    "        x_tfidf_bi = tfidf_vec_bi.fit_transform(dtrain['text'])\n",
    "\n",
    "        res_count_uni = cross_val_score(model,x_count_uni,dtrain['response'],cv=5,scoring='accuracy').mean()\n",
    "        res_tfidf_uni = cross_val_score(model,x_tfidf_uni,dtrain['response'],cv=5,scoring='accuracy').mean()\n",
    "        res_count_bi  = cross_val_score(model,x_count_bi,dtrain['response'],cv=5,scoring='accuracy').mean()\n",
    "        res_tfidf_bi  = cross_val_score(model,x_tfidf_bi,dtrain['response'],cv=5,scoring='accuracy').mean()\n",
    "\n",
    "        print(f'(count,{name},uni)={res_count_uni}')\n",
    "        print(f'(tfidf,{name},uni)={res_tfidf_uni}')\n",
    "        print(f'(count,{name},bi) ={res_count_bi}')\n",
    "        print(f'(tfidf,{name},bi) ={res_tfidf_bi}')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arruracy[C=0.01]=0.6327217591393262\n",
      "Arruracy[C=0.05]=0.732471846761044\n",
      "Arruracy[C=0.1]=0.8207433804785342\n",
      "Arruracy[C=0.25]=0.8424459194402658\n",
      "Arruracy[C=0.5]=0.8464096864992893\n",
      "Arruracy[C=0.75]=0.847868234351632\n",
      "Arruracy[C=0.8]=0.8482859885797881\n",
      "Arruracy[C=0.85]=0.8487033074205256\n",
      "Arruracy[C=0.9]=0.8482859885797879\n",
      "Arruracy[C=0.95]=0.8484951922344299\n",
      "Arruracy[C=1]=0.8474517862857317\n",
      "Arruracy[C=1.25]=0.8472428003247992\n",
      "Arruracy[C=5]=0.8409810584703534\n",
      "Arruracy[C=10]=0.8401462030951692\n"
     ]
    }
   ],
   "source": [
    "for c in [.01, .05, .1, .25, .5, .75, .8, .85, .9, .95, 1, 1.25, 5, 10]:\n",
    "    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_tfidf = tfidf_vec.fit_transform(dtrain['text'])\n",
    "    model = SGDClassifier(C=c, )\n",
    "    \n",
    "    print(f\"Arruracy[C={c}]={cross_val_score(model,X_tfidf,dtrain['response'],cv=5,scoring='accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_iter': (None,), 'alpha': (1e-06, 1e-05, 0.001), 'penalty': ('l2', 'elasticnet'), 'epsilon': (0.5, 0.25, 0.1, 0.01, 0.0001)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_iter': (None,),\n",
    "    'alpha': (1e-6, 1e-5, 1e-3,),\n",
    "    'penalty': ('l2', 'elasticnet'),\n",
    "    'epsilon': (0.5, 0.25, 0.1, 0.01, 1e-4 )\n",
    "}\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_tfidf = tfidf_vec.fit_transform(dtrain['text'])\n",
    "grid_search = GridSearchCV(SGDClassifier(), parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_tfidf, dtrain.response.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-05, 'epsilon': 0.01, 'max_iter': None, 'penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arruracy[n=1]=0.8021313957162384\n",
      "Arruracy[n=11]=0.8523376702754775\n",
      "Arruracy[n=21]=0.8594859991128336\n",
      "Arruracy[n=31]=0.8607085496066841\n",
      "Arruracy[n=41]=0.8622760371319653\n",
      "Arruracy[n=51]=0.8638464129230767\n",
      "Arruracy[n=61]=0.8619295809511701\n",
      "Arruracy[n=71]=0.8668100610068225\n",
      "Arruracy[n=81]=0.8641937812296725\n",
      "Arruracy[n=91]=0.865415421188177\n"
     ]
    }
   ],
   "source": [
    "best_params = {'alpha': 1e-05, 'epsilon': 0.01, 'max_iter': None, 'penalty': 'elasticnet'}\n",
    "for n in list(range(1,100,10)):\n",
    "    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_tfidf = tfidf_vec.fit_transform(dtrain['text'])\n",
    "    model = BaggingClassifier(base_estimator=SGDClassifier(**best_params),n_estimators=n,bootstrap_features=True,bootstrap=True)\n",
    "    \n",
    "    print(f\"Arruracy[n={n}]={cross_val_score(model,X_tfidf,dtrain['response'],cv=5,scoring='accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arruracy[n=200]=0.8661115250623039\n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_tfidf = tfidf_vec.fit_transform(dtrain['text'])\n",
    "model = BaggingClassifier(base_estimator=SGDClassifier(**best_params),n_estimators=n,bootstrap_features=True,bootstrap=True)\n",
    "\n",
    "print(f\"Arruracy[n={n}]={cross_val_score(model,X_tfidf,dtrain['response'],cv=5,scoring='accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator=SGDClassifier(**best_params),n_estimators=n,bootstrap_features=True,bootstrap=True)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "train_matrix = tfidf_vec.fit_transform(dtrain['text'])\n",
    "test_matrix = tfidf_vec.transform(dtest['text'])\n",
    "\n",
    "model.fit(train_matrix, dtrain.response)\n",
    "dtest[\"pred\"]=model.predict(test_matrix)\n",
    "dtest[\"y\"] = dtest.pred.apply(lambda x: \"pos\" if x==1 else \"neg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest[[\"Id\", \"y\"]].to_csv(\"submition03.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1,2))),\n",
    "        (\n",
    "            \"clf\", \n",
    "            BaggingClassifier(\n",
    "                base_estimator=SGDClassifier(\n",
    "                    alpha=1e-05, \n",
    "                    epsilon=0.01,\n",
    "                    penalty='elasticnet'\n",
    "                ),\n",
    "                n_estimators=200,\n",
    "                bootstrap_features=True,\n",
    "                bootstrap=True\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roman/py3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,...mators=200, n_jobs=None, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(dtrain.text,dtrain.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(file=f, obj=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "obj = joblib.load(\"./model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.predict([\"все плохо\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
